# realfast x EQT: External Deck v3
## Designer-Ready Format

*Companion document: EQT_INTERNAL_BRIEF.md (team alignment, not for distribution)*

**Reading guide:**
- `[ON SLIDE]` = What appears on the slide. Keep these elements visual and sparse.
- `[SPEAKER NOTES]` = What you say out loud. Never put this on the slide.
- `[VISUAL]` = A table, diagram, or graphic element to design.
- `[FOOTNOTE]` = Small attribution text, bottom of slide.

---

## Slide 1 — Title

[ON SLIDE]
One AI execution partner.
273 portfolio companies.
Measurable impact in weeks.

realfast — Portfolio-scale AI execution

[SPEAKER NOTES]
None. Let the slide sit for 3-5 seconds. This is your opening image.

---

## Slide 2 — The Adoption Challenge

[ON SLIDE]
Enterprise AI adoption is consistently lower than ambition.
The reasons are structural.

- Initiatives start too broad — no constrained scope
- AI is deployed as a tool, not embedded in how people work
- ROI is measured against assumptions, not operating metrics
- Hidden complexity erodes payback before value is visible

[FOOTNOTE]
40%+ of agentic AI projects will be canceled by 2027 — Gartner | Only 2% of organisations have deployed AI agents at full scale — Capgemini (April 2025) | Only 20% of EQT investment professionals generate the bulk of AI usage — EQT ThinQ, Q1 2025

[SPEAKER NOTES]
"EQT has declared AI adoption existential. You've built Motherbrain, partnered with OpenAI and Anthropic, hired 40 professionals in EQT Digital, and hosted an AI Summit for 75+ tech leaders across your portfolio. The ambition is clear.

But the industry data is equally clear. Over 40% of agentic AI projects will be canceled by 2027. Only 2% of organisations have deployed AI agents at full scale. And your own published data shows that even within EQT, 20% of investment professionals generate the bulk of AI usage.

The gap between ambition and adoption isn't about capability or intent. It's structural. Initiatives start too broad. AI gets bolted on rather than built into how people actually work. ROI gets measured against assumptions instead of real operating metrics. These are the same barriers across every enterprise we work with. And they're solvable — but they require a different execution model."

---

## Slide 3 — The Portfolio Execution Challenge

[ON SLIDE]
EQT is all in on AI. The question is no longer whether — it's how to deploy it across 273 companies.

[ON SLIDE]
Portfolio companies face three unanswered questions:

**Where** — which workflow will produce the fastest, most measurable return?
**How** — who executes it, and how fast can it reach production?
**What to measure** — what's the baseline, and what proves it worked?

[ON SLIDE]
Without answers to these three questions, portfolio companies hit the same five barriers:

1. **No clear starting point** — budget exists, but no method to identify the highest-ROI workflow
2. **Scope too broad** — "AI transformation" instead of one workflow, one metric
3. **No baseline** — without a before measurement, there's no provable after
4. **Vendor fatigue** — 6-12 month engagements that produce strategies, not systems
5. **Internal capacity** — IT teams built for maintenance, not AI-native delivery at speed

[FOOTNOTE]
40%+ of agentic AI projects will be canceled by 2027 — Gartner | Only 2% of organisations have deployed AI agents at full scale — Capgemini (April 2025)

[SPEAKER NOTES]
"EQT has made its position clear — AI is existential, and the investment at the firm level reflects that. This isn't about whether AI matters. That question is settled.

The question now is: how does that conviction translate into production AI systems running inside the workflows of your portfolio companies? And when we look at what those companies face when they try, the same three questions come up every time.

Where do we start? Most companies have a mandate or budget, but no method to identify which specific workflow will produce the fastest, most measurable return. So the initiative starts too broad.

How do we execute? Even when they pick a target, the internal teams are built for maintenance, not for standing up an AI system in weeks. And external vendors are quoting 6 to 12 months and delivering strategy documents, not production systems.

What do we measure? This is the one that kills most projects. Nobody established a baseline before the work started, so when something gets built, there's no before-and-after. The CFO can't point to a number that moved. Without that, there's no case for expanding to the next workflow.

These barriers are structural. They repeat across every mid-market enterprise, regardless of sector. And they're solvable — but they require a specific execution model designed around answering those three questions. That's what we do."

---

## Slide 4 — What We Do

[ON SLIDE]
We are a horizontal AI execution partner.
One proven model, across industries and use cases.

[VISUAL — Three-column framework]

| AUTOMATE | AUGMENT | CONNECT |
|---|---|---|
| Manual work creating backlogs and errors | Decisions made without adequate support | Systems and data that don't talk |
| Extraction, validation, straight-through processing | Scoring, recommendations, draft generation | Cross-system matching, unified views |
| **EBITDA lever:** Cost reduction, throughput | **EBITDA lever:** Revenue quality, decision speed | **EBITDA lever:** Operational efficiency |

[ON SLIDE]
These friction patterns repeat across every industry.
The workflow details change. The underlying barriers are the same.

[SPEAKER NOTES]
"Before we apply AI to anything, we classify why work is slow. Every operational friction falls into one of three categories: work that should be automated, decisions that should be augmented, or systems that need to be connected.

What makes this powerful at portfolio scale is that these patterns repeat across every industry you invest in. A document processing backlog in healthcare is structurally identical to one in compliance services. A decision-latency problem in industrial scheduling is the same class of problem as one in financial underwriting. The workflow details change. The AI execution model doesn't. That's why we work horizontally."

---

## Slide 5 — Why Horizontal Works

[ON SLIDE]
The same friction shows up everywhere.
The industry wrapper changes — the core problem doesn't.

[VISUAL — Matrix grid]

| Friction Pattern | Healthcare | Industrial Services | Tech-Enabled Services | Financial Services |
|---|---|---|---|---|
| Execution backlog | Patient intake | Field service scheduling | Client onboarding | Claims processing |
| Decision latency | Clinical triage | Maintenance scheduling | Pricing and quoting | Underwriting |
| Cost-to-serve | Staff scheduling | Route optimisation | Compliance checking | Reconciliation |
| Data disconnection | Patient records | Asset data across sites | Client data across platforms | Regulatory data |

[ON SLIDE]
One engagement teaches patterns that accelerate the next.
By the third company, we're faster, cheaper, and more accurate.

[SPEAKER NOTES]
"This matrix is the key to why a horizontal model works better than vertical specialists at portfolio scale. Look at any row — the same structural problem appears in every sector. What we learn solving an execution backlog in healthcare directly accelerates how we solve one in industrial services.

By the third portfolio company engagement, we're meaningfully faster and cheaper — because we've already solved the structural problem elsewhere. That's the compounding advantage a horizontal partner delivers that a vertical specialist never can."

---

## Slide 6 — Where We Start

[ON SLIDE]
Not every workflow is worth automating first.
We start where the math works fastest.

[VISUAL — Four selection criteria, each as a card or icon block]

- Direct business metric — something the CFO already tracks
- Repeatable workflow — high frequency = fast proof
- Existing data — no data engineering prerequisite
- Contained blast radius — failure stays local, success is attributable

[ON SLIDE]
Entry points: Execution backlogs | Decision latency | Cost-to-serve pressure | Revenue leakage

[SPEAKER NOTES]
"We're disciplined about where we start. Not every workflow is a good first target. We look for four things: a direct business metric the CFO already tracks, a repeatable workflow with enough volume to generate fast proof, existing data so we're not gated by a data engineering project, and a contained blast radius so that if it doesn't work, it doesn't cascade.

The entry points we're most confident in are execution backlogs — manual processing creating cost and delay; decision-to-execution latency — where the information exists but synthesis is manual; cost-to-serve pressure — where headcount keeps rising just to maintain throughput; and revenue leakage — manual quoting, slow responses, inconsistent pricing.

These aren't limitations. They're the doors that open everything else."

---

## Slide 7 — One Playbook, Every Door

[ON SLIDE]
Same execution model. Regardless of industry, company, or use case.

1. Narrow scope — one workflow, one metric, one team
2. Ship in weeks — production within 3-6 weeks
3. Measure against baseline — before/after on the metric that matters
4. Expand only from proof — next workflow earns its way in

[ON SLIDE]
A repeatable model available to every portfolio company.
Same playbook. Predictable timelines. Predictable costs. Measurable outcomes.

[SPEAKER NOTES]
"This is how we stay horizontal without becoming vague. We don't need to understand your entire business. We need to understand one workflow deeply enough to make it measurably better. Then we move to the next.

For EQT, this means a model you can offer to portfolio companies across your sectors. The portfolio company gets a production AI system. You get a proven value creation lever with predictable costs, timelines, and measurable outcomes."

---

## Slide 8 — What Portfolio Companies Receive

[ON SLIDE]
Tangible outcomes. Not slide decks about AI.

[VISUAL — Four deliverables as cards]

- Production AI system — live, embedded in a real workflow, used by real people
- Measured business impact — before/after on the metric that justified the project
- Operational handover — the company owns and maintains the system
- Expansion roadmap — where to go next, based on proven results

[ON SLIDE]
What the CFO sees: a cost line that dropped, a throughput number that climbed, or a revenue metric that improved — with clear attribution.

[SPEAKER NOTES]
"Every engagement produces four tangible deliverables. A production system that's live and being used. Measured before/after impact on the business metric. Full operational handover so the company owns it. And an expansion roadmap showing where to go next, based on what we've just proved.

What matters for EQT: this creates a repeatable proof point. When the next portfolio company asks 'does AI actually work for businesses like ours?' — you have a measured result from inside your own portfolio to point to."

---

## Slide 9 — The Partnership Model

[ON SLIDE]
How this works at portfolio scale.

[VISUAL — Diagram]

```
                    EQT (Partnership)
                    |
        ┌───────────┼───────────┐
        |           |           |
   Healthcare   Industrials   Services   ...
        |           |           |
   ┌────┴────┐ ┌────┴────┐ ┌────┴────┐
   │realfast │ │realfast │ │realfast │
   │Playbook │ │Playbook │ │Playbook │
   └─────────┘ └─────────┘ └─────────┘
        |           |           |
        └───────────┼───────────┘
                    |
        Cross-Portfolio Intelligence
```

[ON SLIDE]
The flywheel:
1. Select together — identify companies where friction and readiness align
2. We execute — same playbook, adapted to the workflow
3. Results feed back — patterns, benchmarks, proven use cases
4. Deploy to the next — each engagement accelerates the one after it

[SPEAKER NOTES]
"This is the model. We work together to identify portfolio companies where operational friction and readiness align. We execute using our standard playbook, adapted to the specific workflow. Results feed back — cross-portfolio pattern recognition, sector benchmarks, proven use cases. And then we deploy to the next company, where each engagement is faster and more informed because of the one before it.

What makes this compound over time: every engagement deepens the pattern recognition across the portfolio. This builds a cross-portfolio intelligence layer — benchmarks, proven use cases, sector-specific refinements — that gets more valuable with every deployment. That's a moat for both of us."

---

## Slide 10 — The Economics

[ON SLIDE]
Phase model designed for PE risk appetite.

[VISUAL — Phase table]

| Phase | What Happens | Investment | Timeline |
|---|---|---|---|
| Prove | 1 company, 1 workflow | $50-150K | 3-6 weeks |
| Validate | 2-3 companies, cross-sector | $150-400K | 6-12 weeks |
| Scale | Cross-portfolio deployment | Framework pricing | Ongoing |

[ON SLIDE]
We measure in operating metrics, not aspirational percentages.

[VISUAL — Impact patterns table]

| What We Target | Typical Before State | Typical After State |
|---|---|---|
| Manual recurring processes | 1-2 days of team time per cycle | Under 1 hour per cycle |
| Document processing throughput | Hours per document, high error rate | Minutes per document, consistent output |
| Feature delivery on legacy systems | Months-long backlogs, talent scarcity | Weeks to production, existing codebase |
| Revenue-critical delivery deadlines | Missed timelines, customer churn risk | Deadlines met, accounts retained |

[ON SLIDE]
One workflow pays for itself. What follows is expansion — not a second pitch.

Typical pattern: initial engagement surfaces 4-5 additional workflows at the same company.
Payback on first engagement: 2-4 months.

[FOOTNOTE]
Based on delivery patterns across engagements. Actual impact depends on workflow complexity, team size, and data readiness.

[SPEAKER NOTES]
"I'm not going to show you a slide that multiplies assumptions across 273 companies. You can do that math better than I can — you know your portfolio.

What I can show you is what happens at the individual company level when we execute. A finance team that was spending two days every month on manual reconciliation now spends less than an hour. A feature backlog on a legacy system that internal teams estimated at nine months gets compressed to weeks — because the alternative was losing their largest customer.

What's consistent across every engagement is the pattern afterward: one workflow proves the model, and that first engagement typically surfaces four or five additional workflows the company didn't realize were addressable. The compounding happens organically — we don't need to pitch a second engagement. The company comes to us.

The cost to prove this inside one portfolio company is $50 to $150K and 5-6 weeks. The payback on that first engagement is typically 2-4 months. After that, the question you'll be asking isn't whether this works — it's how many companies to deploy it across."

---

## Slide 11 — Why realfast

[ON SLIDE]
Don't take our word for it. Test us.

[VISUAL — Comparison table]

| | realfast | Perficient | Big 4 / SI | Boutique AI |
|---|---|---|---|---|
| Built for | Horizontal portfolio execution | Broad digital transformation | Enterprise advisory | Deep tech R&D |
| Speed | 3-6 weeks | 3-6 months | 6-12 months | Varies |
| Portfolio fit | Same playbook, any industry | Engagement by engagement | Engagement by engagement | Narrow |
| Learning | Compounds cross-portfolio | Standalone | Standalone | No portfolio view |
| Teams | Senior engineers, small | Mixed seniority, large | Junior-heavy, large | Research |
| Pricing | Outcome-anchored | T&M, scope grows | T&M, scope grows | Project |

[ON SLIDE]
Rather than debate it, let's prove it.
Pick a portfolio company. Pick a workflow. Run us alongside your existing options.
Same problem. Same timeline. Compare the output.

[SPEAKER NOTES]
"You've invested heavily in Perficient and that's the right call for large digital programs. We think we're the right call for fast, focused AI execution. But rather than debate the difference, let's demonstrate it.

Pick a portfolio company. Pick a workflow. Run us alongside your existing options — same problem, same timeline. Compare the output on speed, cost, adoption, and measured impact. We're confident enough to invite the comparison."

---

## Slide 12 — How It Starts

[ON SLIDE]
One decision. One company. One workflow.

1. Joint selection — identify a company based on friction and readiness
2. Scoping call — 1 hour. One workflow, one metric, success criteria.
3. Discovery sprint — 2 weeks. Map workflow, establish baseline, design mechanism.
4. Build + deploy — 3-4 weeks. Production system with real users.
5. Measure + decide — hard numbers. Expand, or stop.

[ON SLIDE]
5-6 weeks from handshake to measured impact.

[SPEAKER NOTES]
"The engagement starts with one joint decision — which portfolio company, which workflow. A one-hour scoping call to define the metric and success criteria. Then a two-week discovery sprint to map the workflow, establish a baseline, and design the AI mechanism. Three to four weeks of build and deploy. And then we measure.

Total elapsed time: 5-6 weeks from handshake to measured impact. After Phase 1, the conversation changes from 'will this work?' to 'where do we deploy next?'"

---

## Slide 13 — Close

[ON SLIDE]
Enterprise AI adoption is held back by structural execution barriers — not by ambition or technology.

A portfolio of 273+ companies represents both the challenge and the opportunity.

That starts with one conversation.

[SPEAKER NOTES]
Let this slide breathe. Pause. Then: "We'd like to explore what a Phase 1 engagement could look like with one of your portfolio companies. One conversation to see if the fit is there."

---

# APPENDIX: TALKING POINTS BY AUDIENCE

*These are NOT slides. These are preparation notes for who's in the room.*

## For EQT Partners / Investment Committee

**Lead with:** "The industry data is clear — the gap between AI ambition and AI execution is the defining problem. Your portfolio companies face the same structural barriers every enterprise does: initiatives start too broad, AI gets bolted on rather than built in, and there's no baseline to prove ROI. A horizontal execution partner with a repeatable playbook can systematically address that across the portfolio. The cost to prove it is <$150K. The potential value across even 10 companies is measured in hundreds of millions."

**Partnership frame:** "Think of us the way you think about operational advisors in your industrial network. You don't have 600 advisors on payroll — you have a network you deploy. We're proposing the AI execution equivalent."

**If Perficient comes up:** "Perficient is the right investment for broad digital transformation. What we do is narrower, faster, and cheaper for a specific class of problem. Rather than debate it, let's demonstrate it. Run us on the same problem, same timeline."

## For Sven Tornkvist / Petter Weiderholm (EQT Digital)

**Lead with:** "We're not here to do what you do — we don't do strategy, digital maturity, or technology selection. We do one thing: take a specific workflow inside a portfolio company and put an AI system into production around it in 5-6 weeks, measured against a real operating metric. We think there's an opportunity to work together across the portfolio."

**If Perficient comes up:** "Happy to run side by side. Same problem, same timeline. We think the results show different tools for different jobs."

**Cross-sector question:** "The friction patterns are structurally similar across sectors. A document processing backlog in healthcare is the same class of problem as one in compliance services. The workflow wrapper changes. The execution model doesn't."

## For Portfolio Company CEOs

**Lead with:** "EQT has introduced us to help your team get one AI system into production — targeting a workflow that's creating cost or delays. 5-6 week engagement. We pick one workflow together, measure where it is today, build an AI system around it, and measure the after. You keep the system."

**If bad AI experiences:** "Most AI initiatives stall for the same reason: too broad, not anchored to a specific operating metric, and bolted on to an existing process rather than built into how people actually work. We do it differently — one narrow workflow, one number the CFO already tracks, embedded in the way your team already operates."

---

# VERIFIED SOURCE DATA

| Metric | Source | Verified |
|---|---|---|
| EQT AUM: EUR 270B | EQT Year-End Report 2025 | Yes |
| Portfolio: 273+ companies | EQT website | Yes |
| EQT Digital: ~40 professionals | EQT publications | Yes |
| 20% of investment professionals generate bulk of AI usage | Tornkvist & Weiderholm, EQT ThinQ, Q1 2025. Exact: "only 20 percent of investment professional users represent the lion's share of AI-connected prompts during Q1 2025" | Yes |
| 40%+ agentic AI projects canceled by 2027 | Gartner, cited in EQT ThinQ "Taking AI From Pilot to Production Scale" | Yes |
| 2% of orgs deployed AI agents at full scale | Capgemini survey (April 2025), cited in same EQT ThinQ article | Yes |
| Perficient acquisition: $3B (Oct 2024) | Press release | Yes |
| 94% of exit value from operational improvement | EQT publications | Yes |
| EQT Industrial Network: 600+ advisors | EQT website | Yes |
| EQT X fund: EUR 22B | EQT filings | Yes |
